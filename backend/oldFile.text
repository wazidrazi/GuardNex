# import os
# from datetime import datetime, timezone, timedelta
# from flask import Flask, request, jsonify
# from flask_cors import CORS
# from werkzeug.security import generate_password_hash, check_password_hash
# import jwt.api_jwt as jwt
# from functools import wraps
# import psycopg2
# from psycopg2.extras import RealDictCursor
# import numpy as np
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.pipeline import Pipeline
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import classification_report, accuracy_score
# import pandas as pd
# import json
# import re
# import unicodedata
# from collections import defaultdict

# app = Flask(__name__)
# # Configure CORS to allow requests from the frontend with all necessary headers
# CORS(app, resources={
#     r"/api/*": {
#         "origins": ["http://localhost:5173", "http://localhost:3000"],
#         "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
#         "allow_headers": ["Content-Type", "Authorization"],
#         "supports_credentials": True
#     }
# })

# # Configuration
# app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev_key_please_change_in_production')
# app.config['DATABASE_URL'] = os.environ.get(
#     'DATABASE_URL',
#     'postgresql://postgres:Mango%40292@localhost:5432/spam_detection'
# )
# app.config['JWT_EXPIRATION'] = int(os.environ.get('JWT_EXPIRATION', 86400))  # 24 hours

# # Global model variable
# spam_model = None

# # Database connection
# def get_db_connection():
#     conn = psycopg2.connect(app.config['DATABASE_URL'])
#     conn.autocommit = True
#     return conn

# # Initialize database tables if they don't exist
# def init_db():
#     conn = get_db_connection()
#     cursor = conn.cursor()
    
#     # Create users table
#     cursor.execute('''
#     CREATE TABLE IF NOT EXISTS users (
#         id SERIAL PRIMARY KEY,
#         name VARCHAR(100) NOT NULL,
#         email VARCHAR(100) UNIQUE NOT NULL,
#         password VARCHAR(255) NOT NULL,
#         role VARCHAR(20) DEFAULT 'user',
#         created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
#     )
#     ''')
    
#     # Create messages table with enhanced fields
#     cursor.execute('''
#     CREATE TABLE IF NOT EXISTS messages (
#         id SERIAL PRIMARY KEY,
#         user_id INTEGER REFERENCES users(id),
#         content TEXT NOT NULL,
#         type VARCHAR(20) NOT NULL,
#         language VARCHAR(10) DEFAULT 'unknown',
#         is_spam BOOLEAN NOT NULL,
#         confidence FLOAT NOT NULL,
#         spam_indicators JSONB DEFAULT '{}',
#         created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
#     )
#     ''')
    
#     # Check if admin exists, if not create one
#     cursor.execute("SELECT * FROM users WHERE email = 'admin@example.com'")
#     admin = cursor.fetchone()
    
#     if not admin:
#         admin_password = generate_password_hash('admin123')
#         cursor.execute(
#             "INSERT INTO users (name, email, password, role) VALUES (%s, %s, %s, %s)",
#             ('Admin User', 'admin@example.com', admin_password, 'admin')
#         )
    
#     conn.commit()
#     cursor.close()
#     conn.close()

# # Enhanced text preprocessing for multi-language support
# class MultiLanguagePreprocessor:
#     def __init__(self):
#         # Common spam keywords in different languages
#         self.spam_keywords = {
#         'bangla': [
#             # Existing ones
#             '‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá', '‡¶´‡ßç‡¶∞‡¶ø', '‡¶ú‡¶ø‡¶§‡ßÅ‡¶®', '‡¶™‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡¶æ‡¶∞', '‡¶≤‡¶ü‡¶æ‡¶∞‡¶ø', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶Ö‡¶´‡¶æ‡¶∞',
#             '‡¶õ‡¶æ‡¶°‡¶º', '‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶ï‡ßç‡¶≤‡¶ø‡¶ï', '‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø', '‡¶¨‡¶ø‡¶ú‡¶Ø‡¶º‡ßÄ',
#             '‡¶≤‡¶æ‡¶≠‡¶ú‡¶®‡¶ï', '‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó', '‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§', '‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®', '‡¶™‡ßá‡¶§‡ßá',
#             '‡¶∞‡¶ø‡¶ö‡¶æ‡¶∞‡ßç‡¶ú', '‡¶≤‡¶ï‡ßç‡¶∑', '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶ï‡ßã‡¶ü‡¶ø', '‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü', '‡¶¨‡¶®‡ßç‡¶ß', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ',
#             '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑', '‡¶°‡¶ø‡¶∏‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü', '‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤', '‡¶´‡ßã‡¶®', '‡¶®‡¶Æ‡ßç‡¶¨‡¶∞', '‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏',

#             # New additions (more realistic spam triggers)
#             '‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶ø', '‡¶ã‡¶£', '‡¶á‡¶è‡¶Æ‡¶Ü‡¶á', '‡¶ß‡¶æ‡¶∞', '‡¶´‡¶æ‡¶®‡ßç‡¶°', '‡¶™‡ßç‡¶∞‡¶´‡¶ø‡¶ü',
#             '‡¶°‡¶ø‡¶™‡ßã‡¶ú‡¶ø‡¶ü', '‡¶∏‡¶û‡ßç‡¶ö‡¶Ø‡¶º', '‡¶á‡¶®‡¶ï‡¶æ‡¶Æ', '‡¶Ü‡¶Ø‡¶º', '‡¶Ö‡¶∞‡ßç‡¶•',
#             '‡¶™‡ßá‡¶Æ‡ßá‡¶®‡ßç‡¶ü', '‡¶≤‡ßá‡¶®‡¶¶‡ßá‡¶®', '‡¶ï‡ßç‡¶Ø‡¶æ‡¶∂', '‡¶°‡¶≤‡¶æ‡¶∞', '‡¶∞‡ßá‡¶Æ‡¶ø‡¶ü‡ßá‡¶®‡ßç‡¶∏',
#             '‡¶ü‡¶æ‡¶ï‡¶æ ‡¶™‡¶æ‡¶†‡¶æ‡¶®', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞', '‡¶≠‡ßá‡¶∞‡¶ø‡¶´‡¶æ‡¶á', '‡¶™‡¶æ‡¶∏‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶°', '‡¶ì‡¶ü‡¶ø‡¶™‡¶ø',
#             '‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§ ‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶è‡¶ï‡ßç‡¶∏‡¶ï‡ßç‡¶≤‡ßÅ‡¶∏‡¶ø‡¶≠', '‡¶¨‡ßã‡¶®‡¶æ‡¶∏', '‡¶™‡ßç‡¶∞‡¶æ‡¶á‡¶ú',
#             '‡¶≤‡¶æ‡¶≠', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ', '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø', '‡¶™‡¶æ‡¶∞‡ßç‡¶ü ‡¶ü‡¶æ‡¶á‡¶Æ', '‡¶Ü‡¶Ø‡¶º ‡¶ï‡¶∞‡ßÅ‡¶®',
#             '‡¶∏‡ßã‡¶®‡¶æ', '‡¶π‡ßÄ‡¶∞‡¶æ‡¶∞', '‡¶™‡ßç‡¶∞‡¶™‡¶æ‡¶∞‡ßç‡¶ü‡¶ø', '‡¶™‡ßç‡¶≤‡¶ü', '‡¶´‡ßç‡¶≤‡ßç‡¶Ø‡¶æ‡¶ü',
#             '‡¶∂‡ßá‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶∏‡ßç‡¶ü‡¶ï', '‡¶¨‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó',
#             '‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®', '‡¶≤‡¶æ‡¶á‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶∂‡ßá‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶∏‡¶æ‡¶¨‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶æ‡¶á‡¶¨',
#             '‡¶≠‡¶ø‡¶∏‡¶æ', '‡¶Æ‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßç‡¶°', '‡¶ï‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ü ‡¶ï‡¶æ‡¶∞‡ßç‡¶°', '‡¶°‡ßá‡¶¨‡¶ø‡¶ü ‡¶ï‡¶æ‡¶∞‡ßç‡¶°',
#             '‡¶≤‡¶ø‡¶Æ‡¶ø‡¶ü‡ßá‡¶° ‡¶Ö‡¶´‡¶æ‡¶∞', '‡¶´‡ßá‡¶ï', '‡¶Ö‡¶¨‡¶ø‡¶≤‡¶Æ‡ßç‡¶¨‡ßá', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°',
#             '‡¶Ö‡¶∞‡¶ø‡¶ú‡¶ø‡¶®‡¶æ‡¶≤ ‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü', '‡¶∏‡¶∏‡ßç‡¶§‡¶æ ‡¶¶‡¶æ‡¶Æ‡ßá', '‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶®', '‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®',
#             '‡ßß‡ß¶‡ß¶% ‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶Ü‡¶Ø‡¶º', '‡¶Ü‡¶ú‡¶ï‡ßá‡¶á', '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞',
#             '‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø', '‡¶≠‡¶ø‡¶∏‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç', '‡¶´‡ßç‡¶∞‡¶ø ‡¶≠‡¶ø‡¶∏‡¶æ', '‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶ï ‡¶™‡¶æ‡¶∞‡¶Æ‡¶ø‡¶ü',
#             '‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶´‡ßç‡¶∞‡¶ø', '‡¶∏‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶ü', '‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶Æ‡¶ø‡¶∂‡¶®', '‡¶´‡ßç‡¶∞‡¶ø ‡¶∞‡ßá‡¶ú‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡ßá‡¶∂‡¶®',
#             '‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶®‡ßá‡¶ü ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï', '‡¶´‡ßç‡¶∞‡¶ø ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü', '‡¶´‡ßç‡¶∞‡¶ø ‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏', '‡¶´‡ßç‡¶∞‡¶ø ‡¶è‡¶Æ‡¶¨‡¶ø',
#             '‡¶ú‡¶Ø‡¶º‡ßá‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø ‡¶π‡ßã‡¶®', '‡¶≠‡¶ø‡¶Ü‡¶á‡¶™‡¶ø ‡¶Ö‡¶´‡¶æ‡¶∞'
#         ]
#     }

#         # Enhanced patterns for Bengali text
#         self.patterns = {
#             'phone': re.compile(r'(\+?88)?[-\s]?01[3-9]\d{8}|(\+?\d{1,3}[-.\s]?)?\(?\d{3,4}\)?[-.\s]?\d{3,4}[-.\s]?\d{3,6}'),
#             'bangla_numbers': re.compile(r'[‡ß¶-‡ßØ]+'),
#             'money_bangla': re.compile(r'‡ß≥\s*[\d‡ß¶-‡ßØ]+|[\d‡ß¶-‡ßØ]+\s*(‡¶ü‡¶æ‡¶ï‡¶æ|‡¶π‡¶æ‡¶ú‡¶æ‡¶∞|‡¶≤‡¶ï‡ßç‡¶∑|‡¶≤‡¶æ‡¶ñ|‡¶ï‡ßã‡¶ü‡¶ø)'),
#             'urgent_bangla': re.compile(r'(‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø|‡¶è‡¶ñ‡¶®‡¶á|‡¶§‡¶æ‡¶°‡¶º‡¶æ‡¶§‡¶æ‡¶°‡¶º‡¶ø|‡¶¶‡ßç‡¶∞‡ßÅ‡¶§|‡¶Ö‡¶¨‡¶ø‡¶≤‡¶Æ‡ßç‡¶¨‡ßá)', re.IGNORECASE)
#         }

#     def detect_language(self, text):
#         """Enhanced language detection for Bengali text"""
#         # Count Bengali characters
#         bengali_chars = len(re.findall(r'[\u0980-\u09FF]', text))
#         # Count English letters
#         english_chars = len(re.findall(r'[a-zA-Z]', text))
#         # Count total meaningful characters
#         total_chars = len(re.sub(r'[\s\d\W]', '', text))
        
#         if total_chars == 0:
#             return 'unknown'
        
#         bengali_ratio = bengali_chars / max(total_chars, 1)
#         english_ratio = english_chars / max(total_chars, 1)
        
#         # Adjusted thresholds for better Bengali detection
#         if bengali_ratio > 0.3:  # Lowered threshold for Bengali
#             return 'bangla'
#         elif english_ratio > 0.6:
#             return 'english'
#         elif bengali_ratio > 0.1 and english_ratio > 0.1:
#             return 'mixed'
#         else:
#             return 'unknown'

#     def extract_features(self, text):
#         """Extract spam indicators from text"""
#         features = defaultdict(int)
#         text_lower = text.lower()
        
#         # Language detection
#         language = self.detect_language(text)
#         features['language'] = language
        
#         # Keyword matching
#         for lang, keywords in self.spam_keywords.items():
#             for keyword in keywords:
#                 if keyword in text_lower:
#                     features[f'{lang}_spam_keywords'] += 1
        
#         # Pattern matching
#         for pattern_name, pattern in self.patterns.items():
#             matches = pattern.findall(text)
#             features[f'{pattern_name}_count'] = len(matches)
        
#         # Text statistics
#         features['length'] = len(text)
#         features['word_count'] = len(text.split())
#         features['caps_ratio'] = len(re.findall(r'[A-Z]', text)) / max(len(text), 1)
#         features['digit_ratio'] = len(re.findall(r'\d', text)) / max(len(text), 1)
#         features['punctuation_ratio'] = len(re.findall(r'[!@#$%^&*(),.?":{}|<>]', text)) / max(len(text), 1)
        
#         return dict(features)

#     def preprocess_text(self, text):
#         """Clean and normalize text"""
#         # Remove extra whitespace
#         text = re.sub(r'\s+', ' ', text).strip()
        
#         # Keep original text for multi-language TF-IDF (no Unicode normalization)
#         return text

# # Enhanced model training with multi-language support
# def create_enhanced_dataset():
#     """Create a comprehensive dataset by importing multiple CSV files"""
#     print("Loading datasets from CSV files...")
    
#     try:
#         # Check if CSV files exist
#         required_files = ['emails.csv', 'Bangla_Email_Dataset.csv', 'Dataset_5971.csv']
#         for file in required_files:
#             if not os.path.exists(file):
#                 print(f"Warning: {file} not found!")
                
#         # Load English emails dataset
#         emails_df = pd.read_csv('emails.csv')
#         print(f"Loaded {len(emails_df)} records from emails.csv")
#         print(f"Columns in emails.csv: {emails_df.columns.tolist()}")
        
#         # Load Bengali dataset
#         bangla_df = pd.read_csv('Bangla_Email_Dataset.csv')
#         print(f"Loaded {len(bangla_df)} records from Bangla_Email_Dataset.csv")
#         print(f"Columns in Bangla_Email_Dataset.csv: {bangla_df.columns.tolist()}")
        
#         # Load additional dataset
#         additional_df = pd.read_csv('Dataset_5971.csv')
#         print(f"Loaded {len(additional_df)} records from Dataset_5971.csv")
#         print(f"Columns in Dataset_5971.csv: {additional_df.columns.tolist()}")
        
#         # Combine and preprocess datasets
#         combined_data = {
#             'message': [],
#             'label': []
#         }
        
#         # Process English emails
#         for _, row in emails_df.iterrows():
#             combined_data['message'].append(row['Subject'])  # Adjust column name if different
#             combined_data['label'].append(int(row['Label']))  # Adjust column name if different
            
#         # Process Bengali emails
#         for _, row in bangla_df.iterrows():
#             combined_data['message'].append(row['Text'])  # Adjust column name if different
#             combined_data['label'].append(int(row['Label']))  # Adjust column name if different
            
#         # Process additional dataset
#         for _, row in additional_df.iterrows():
#             combined_data['message'].append(row['Message'])  # Adjust column name if different
#             combined_data['label'].append(int(row['Label']))  # Adjust column name if different
        
#         # Create combined DataFrame
#         df = pd.DataFrame(combined_data)
        
#         # Remove any null values
#         df = df.dropna()
        
#         # Save combined dataset
#         df.to_csv('spam_dataset.csv', index=False)
        
#         print(f"Combined dataset created with {len(df)} messages")
#         print(f"Spam: {sum(df['label'])}, Ham: {len(df) - sum(df['label'])}")
        
#         return df
        
#     except Exception as e:
#         print(f"Error loading datasets: {e}")
#         print("Falling back to sample dataset...")
        
#         # Fallback to existing sample data if CSV loading fails
#         if os.path.exists('spam_dataset.csv'):
#             df = pd.read_csv('spam_dataset.csv')
#             print(f"Loaded existing dataset with {len(df)} messages")
#             return df
#         else:
#             # Create sample dataset as fallback
#             sample_data = {
#                 'message': [
#                     # Your existing sample data...
#                 ],
#                 'label': [
#                     # Your existing labels...
#                 ]
#             }
#             df = pd.DataFrame(sample_data)
#             df.to_csv('spam_dataset.csv', index=False)
#             print(f"Created fallback dataset with {len(df)} messages")
#             return df
# def load_or_train_model():
#     """Load or train model with enhanced Bangla support"""
#     print("Training enhanced multi-language spam detection model...")
    
#     try:
#         # Load datasets
#         print("Loading datasets...")
#         bangla_df = pd.read_csv('Bangla_Email_Dataset.csv')
        
#         # Ensure correct column names
#         if 'Text' in bangla_df.columns and 'Label' in bangla_df.columns:
#             bangla_df = bangla_df.rename(columns={'Text': 'message', 'Label': 'label'})
        
#         # Initialize preprocessor
#         preprocessor = BanglaTextPreprocessor()
        
#         # Preprocess texts
#         print("Preprocessing Bangla texts...")
#         bangla_df['processed_text'] = bangla_df['message'].apply(preprocessor.preprocess_text)
        
#         # Extract features
#         print("Extracting features...")
#         features_list = []
#         for text in bangla_df['message']:
#             features = preprocessor.extract_features(text)
#             features_list.append(features)
        
#         # Convert features to DataFrame
#         features_df = pd.DataFrame(features_list)
        
#         # Combine features with TF-IDF
#         print("Creating TF-IDF features...")
#         tfidf = TfidfVectorizer(
#             max_features=10000,
#             ngram_range=(1, 3),
#             min_df=2,
#             max_df=0.9,
#             analyzer='char_wb',  # Use character n-grams for better Bangla support
#             strip_accents=None,
#             lowercase=False  # Don't lowercase for Bangla
#         )
        
#         # Create TF-IDF features
#         tfidf_features = tfidf.fit_transform(bangla_df['processed_text'])
        
#         # Convert to dense array and combine with other features
#         X = np.hstack([
#             tfidf_features.toarray(),
#             features_df.values
#         ])
        
#         y = bangla_df['label']
        
#         # Split data
#         X_train, X_test, y_train, y_test = train_test_split(
#             X, y, test_size=0.2, random_state=42, stratify=y
#         )
        
#         # Train model
#         print("Training model...")
#         model = RandomForestClassifier(
#             n_estimators=200,
#             max_depth=30,
#             min_samples_split=3,
#             class_weight='balanced',
#             random_state=42
#         )
        
#         model.fit(X_train, y_train)
        
#         # Evaluate
#         y_pred = model.predict(X_test)
#         accuracy = accuracy_score(y_test, y_pred)
#         print(f"Model accuracy: {accuracy:.2%}")
#         print("\nClassification Report:")
#         print(classification_report(y_test, y_pred))
        
#         return model, preprocessor
        
#     except Exception as e:
#         print(f"Error during model training: {e}")
#         return create_fallback_model()

# def create_fallback_model():
#     """Create a simple fallback model if main training fails"""
#     print("Creating fallback model...")
    
#     # Simple model with basic data
#     simple_data = {
#         'message': [
#             'win free money now', 'click here urgent', 'congratulations winner',
#             'hello how are you', 'meeting at 3pm', 'thanks for help'
#         ],
#         'label': [1, 1, 1, 0, 0, 0]
#     }
    
#     df = pd.DataFrame(simple_data)
#     model = Pipeline([
#         ('tfidf', TfidfVectorizer(stop_words='english', max_features=1000)),
#         ('classifier', MultinomialNB())
#     ])
    
#     model.fit(df['message'], df['label'])
#     preprocessor = MultiLanguagePreprocessor()
    
#     return model, preprocessor

# # Enhanced prediction function
# def predict_message(message):
#     """Enhanced prediction for Bangla text"""
#     global spam_model, text_preprocessor
    
#     if spam_model is None or text_preprocessor is None:
#         return {
#             'isSpam': False,
#             'confidence': 0.0,
#             'message': 'Model not loaded',
#             'language': 'unknown',
#             'indicators': {}
#         }
    
#     try:
#         # Preprocess message
#         processed_text = text_preprocessor.preprocess_text(message)
        
#         # Extract features
#         features = text_preprocessor.extract_features(message)
        
#         # Create TF-IDF features
#         tfidf_features = tfidf_vectorizer.transform([processed_text])
        
#         # Combine features
#         X = np.hstack([
#             tfidf_features.toarray(),
#             np.array([list(features.values())])
#         ])
        
#         # Make prediction
#         prediction = spam_model.predict(X)[0]
#         probabilities = spam_model.predict_proba(X)[0]
#         confidence = float(probabilities[1] if prediction else probabilities[0])
        
#         return {
#             'isSpam': bool(prediction),
#             'confidence': confidence,
#             'message': 'Spam detected' if prediction else 'Not spam',
#             'language': 'bangla' if any('\u0980' <= c <= '\u09FF' for c in message) else 'english',
#             'indicators': {
#                 'spam_keywords': features.get('spam_keyword_count', 0),
#                 'phone_numbers': features.get('phone_bd', 0) + features.get('phone_en', 0),
#                 'money_mentions': features.get('money_bangla', 0),
#                 'urgent_words': features.get('urgent_bangla', 0),
#                 'urls': features.get('url_count', 0),
#                 'text_length': features.get('text_length', 0),
#                 'bangla_numbers': features.get('bangla_number_count', 0)
#             }
#         }
        
#     except Exception as e:
#         print(f"Prediction error: {e}")
#         return {
#             'isSpam': False,
#             'confidence': 0.0,
#             'message': 'Prediction failed',
#             'language': 'unknown',
#             'indicators': {}
#         }

# # JWT Token Required Decorator
# def token_required(f):
#     @wraps(f)
#     def decorated(*args, **kwargs):
#         token = None
        
#         if 'Authorization' in request.headers:
#             auth_header = request.headers['Authorization']
#             if auth_header.startswith('Bearer '):
#                 token = auth_header[7:]
        
#         if not token:
#             return jsonify({'message': 'Token is missing!'}), 401
        
#         try:
#             data = jwt.decode(
#                 jwt=token,
#                 key=app.config['SECRET_KEY'],
#                 algorithms=["HS256"]
#             )
            
#             conn = get_db_connection()
#             cursor = conn.cursor(cursor_factory=RealDictCursor)
#             cursor.execute("SELECT * FROM users WHERE id = %s", (data['user_id'],))
#             current_user = cursor.fetchone()
#             cursor.close()
#             conn.close()
            
#             if not current_user:
#                 return jsonify({'message': 'User no longer exists!'}), 401
            
#         except Exception as e:
#             return jsonify({'message': 'Token is invalid!', 'error': str(e)}), 401
        
#         return f(current_user, *args, **kwargs)
    
#     return decorated

# # Admin Required Decorator
# def admin_required(f):
#     @wraps(f)
#     def decorated(current_user, *args, **kwargs):
#         if current_user['role'] != 'admin':
#             return jsonify({'message': 'Admin privileges required!'}), 403
#         return f(current_user, *args, **kwargs)
    
#     return decorated

# # Routes
# @app.route('/api/auth/register', methods=['POST'])
# def register():
#     data = request.get_json()
    
#     name = data.get('name')
#     email = data.get('email')
#     password = data.get('password')
    
#     if not name or not email or not password:
#         return jsonify({'message': 'Missing required fields'}), 400
    
#     hashed_password = generate_password_hash(password)
    
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor()
        
#         # Check if email already exists
#         cursor.execute("SELECT * FROM users WHERE email = %s", (email,))
#         if cursor.fetchone():
#             return jsonify({'message': 'Email already registered'}), 409
        
#         # Insert new user
#         cursor.execute(
#             "INSERT INTO users (name, email, password) VALUES (%s, %s, %s) RETURNING id",
#             (name, email, hashed_password)
#         )
        
#         user_id = cursor.fetchone()[0]
        
#         conn.commit()
#         cursor.close()
#         conn.close()
        
#         return jsonify({'message': 'User registered successfully', 'user_id': user_id}), 201
    
#     except Exception as e:
#         return jsonify({'message': 'Registration failed', 'error': str(e)}), 500

# @app.route('/api/auth/login', methods=['POST'])
# def login():
#     data = request.get_json()
    
#     email = data.get('email')
#     password = data.get('password')
    
#     if not email or not password:
#         print(f"Login attempt failed: Missing email or password")
#         return jsonify({'message': 'Missing email or password'}), 400
    
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor(cursor_factory=RealDictCursor)
        
#         cursor.execute("SELECT * FROM users WHERE email = %s", (email,))
#         user = cursor.fetchone()
        
#         cursor.close()
#         conn.close()
        
#         if not user:
#             print(f"Login attempt failed: No user found with email {email}")
#             return jsonify({'message': 'Invalid credentials'}), 401
            
#         if not check_password_hash(user['password'], password):
#             print(f"Login attempt failed: Invalid password for user {email}")
#             return jsonify({'message': 'Invalid credentials'}), 401
        
#         # Generate JWT token with timezone-aware datetime
#         token_payload = {
#             'user_id': user['id'],
#             'email': user['email'],
#             'role': user['role'],
#             'exp': datetime.now(timezone.utc) + timedelta(seconds=app.config['JWT_EXPIRATION'])
#         }
        
#         # Use jwt.encode with explicit algorithm
#         token = jwt.encode(
#             payload=token_payload,
#             key=app.config['SECRET_KEY'],
#             algorithm="HS256"
#         )
        
#         print(f"Successful login for user {email} with role {user['role']}")
        
#         return jsonify({
#             'token': token,
#             'user': {
#                 'id': user['id'],
#                 'name': user['name'],
#                 'email': user['email'],
#                 'role': user['role']
#             }
#         }), 200
    
#     except Exception as e:
#         print(f"Login error: {str(e)}")
#         return jsonify({'message': 'Login failed', 'error': str(e)}), 500

# @app.route('/api/predict', methods=['POST'])
# @token_required
# def predict_spam(current_user):
#     try:
#         data = request.get_json()
        
#         if not data or not data.get('message'):
#             return jsonify({'error': 'No message provided'}), 400
            
#         message = data.get('message')
#         message_type = data.get('type', 'email')

#         # Initialize preprocessor if needed
#         if not hasattr(app, 'text_preprocessor'):
#             app.text_preprocessor = MultiLanguagePreprocessor()

#         # Detect language and preprocess
#         is_bangla = any('\u0980' <= c <= '\u09FF' for c in message)
#         language = 'bangla' if is_bangla else 'english'
        
#         # Spam detection logic
#         spam_score = 0
#         confidence = 0.5
#         indicators = {
#             'spam_keywords': 0,
#             'phone_numbers': 0,
#             'money_mentions': 0,
#             'urgent_words': 0,
#             'urls': 0,
#             'text_length': len(message)
#         }

#         if is_bangla:
#             # Bangla spam detection
#             bangla_spam_keywords = [
#                 '‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá', '‡¶´‡ßç‡¶∞‡¶ø', '‡¶ú‡¶ø‡¶§‡ßÅ‡¶®', '‡¶™‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡¶æ‡¶∞', '‡¶≤‡¶ü‡¶æ‡¶∞‡¶ø', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶Ö‡¶´‡¶æ‡¶∞',
#                 '‡¶õ‡¶æ‡¶°‡¶º', '‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶ï‡ßç‡¶≤‡¶ø‡¶ï', '‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø', '‡¶¨‡¶ø‡¶ú‡¶Ø‡¶º‡ßÄ',
#                 '‡¶≤‡¶æ‡¶≠‡¶ú‡¶®‡¶ï', '‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó', '‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§', '‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®'
#             ]
            
#             # Check for Bangla spam keywords
#             for keyword in bangla_spam_keywords:
#                 if keyword in message.lower():
#                     spam_score += 1
#                     indicators['spam_keywords'] += 1

#             # Check for phone numbers
#             phone_patterns = [
#                 r'(\+?88)?[-\s]?01[3-9]\d{8}',  # Bangla mobile numbers
#                 r'[‡ß¶-‡ßØ]{11}'  # Bangla numeric mobile numbers
#             ]
#             for pattern in phone_patterns:
#                 if re.search(pattern, message):
#                     spam_score += 1
#                     indicators['phone_numbers'] += 1

#             # Check for money mentions
#             money_patterns = [
#                 r'‡ß≥\s*[\d‡ß¶-‡ßØ]+',
#                 r'[\d‡ß¶-‡ßØ]+\s*(‡¶ü‡¶æ‡¶ï‡¶æ|‡¶π‡¶æ‡¶ú‡¶æ‡¶∞|‡¶≤‡¶ï‡ßç‡¶∑|‡¶≤‡¶æ‡¶ñ|‡¶ï‡ßã‡¶ü‡¶ø)'
#             ]
#             for pattern in money_patterns:
#                 if re.search(pattern, message):
#                     spam_score += 1
#                     indicators['money_mentions'] += 1

#             # Check for urgent words
#             urgent_words = ['‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶§‡¶æ‡¶°‡¶º‡¶æ‡¶§‡¶æ‡¶°‡¶º‡¶ø', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§']
#             for word in urgent_words:
#                 if word in message.lower():
#                     spam_score += 1
#                     indicators['urgent_words'] += 1

#         else:
#             # English spam detection
#             english_spam_keywords = [
#                 'free', 'win', 'winner', 'cash', 'prize', 'urgent', 'offer',
#                 'limited', 'click', 'call now', 'guarantee', 'discount',
#                 'congratulation', 'lucky', 'selected', 'money', 'loan'
#             ]
            
#             # Check for English spam keywords
#             for keyword in english_spam_keywords:
#                 if keyword in message.lower():
#                     spam_score += 1
#                     indicators['spam_keywords'] += 1

#             # Check for phone numbers
#             if re.search(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', message):
#                 spam_score += 1
#                 indicators['phone_numbers'] += 1

#             # Check for money mentions
#             if re.search(r'[$‚Ç¨¬£]\s*\d+|\d+\s*(?:dollars|euro|pound|USD|EUR|GBP)', message.lower()):
#                 spam_score += 1
#                 indicators['money_mentions'] += 1

#             # Check for urgent words
#             urgent_words = ['urgent', 'now', 'hurry', 'limited time', 'act now', 'today only']
#             for word in urgent_words:
#                 if word in message.lower():
#                     spam_score += 1
#                     indicators['urgent_words'] += 1

#         # Check for URLs in both languages
#         if re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', message):
#             spam_score += 1
#             indicators['urls'] += 1

#         # Calculate final spam probability
#         is_spam = spam_score >= 2  # Consider spam if 2 or more indicators
#         confidence = min(0.5 + (spam_score * 0.1), 0.99) if is_spam else max(0.5 - (spam_score * 0.1), 0.01)

#         result = {
#             'isSpam': is_spam,
#             'confidence': confidence,
#             'message': 'Spam detected' if is_spam else 'Not spam',
#             'language': language,
#             'indicators': indicators
#         }

#         # Save to database
#         try:
#             conn = get_db_connection()
#             cursor = conn.cursor()
#             cursor.execute(
#                 """INSERT INTO messages 
#                    (user_id, content, type, language, is_spam, confidence, spam_indicators) 
#                    VALUES (%s, %s, %s, %s, %s, %s, %s)""",
#                 (current_user['id'], message, message_type, language,
#                  is_spam, confidence, json.dumps(indicators))
#             )
#             conn.commit()
#             cursor.close()
#             conn.close()
#         except Exception as db_error:
#             print(f"Database error: {db_error}")

#         return jsonify(result), 200

#     except Exception as e:
#         print(f"Prediction error: {str(e)}")
#         return jsonify({
#             'error': 'Error detecting spam',
#             'details': str(e)
#         }), 500
# # Admin routes (keeping existing ones and adding new ones)
# @app.route('/api/admin/stats', methods=['GET'])
# @token_required
# @admin_required
# def admin_stats(current_user):
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor(cursor_factory=RealDictCursor)
        
#         # Get total users
#         cursor.execute("SELECT COUNT(*) as total FROM users")
#         total_users = cursor.fetchone()['total']
        
#         # Get total messages
#         cursor.execute("SELECT COUNT(*) as total FROM messages")
#         total_messages = cursor.fetchone()['total']
        
#         # Get spam and ham counts
#         cursor.execute("""
#             SELECT 
#                 COUNT(*) FILTER (WHERE is_spam = true) as spam_count,
#                 COUNT(*) FILTER (WHERE is_spam = false) as ham_count
#             FROM messages
#         """)
#         counts = cursor.fetchone()
        
#         # Get messages by type
#         cursor.execute("""
#             SELECT type, COUNT(*) as count 
#             FROM messages 
#             GROUP BY type
#         """)
#         messages_by_type = {row['type']: row['count'] for row in cursor.fetchall()}
        
#         # Get recent activity
#         cursor.execute("""
#             SELECT 
#                 m.id,
#                 m.content,
#                 m.type,
#                 m.is_spam,
#                 m.confidence,
#                 m.created_at,
#                 json_build_object(
#                     'id', u.id,
#                     'name', u.name,
#                     'email', u.email
#                 ) as user
#             FROM messages m
#             JOIN users u ON m.user_id = u.id
#             ORDER BY m.created_at DESC
#             LIMIT 10
#         """)
#         recent_activity = cursor.fetchall()
        
#         # Get message trends (last 7 days)
#         cursor.execute("""
#             SELECT 
#                 DATE(created_at) as date,
#                 COUNT(*) FILTER (WHERE is_spam = true) as spam_count,
#                 COUNT(*) FILTER (WHERE is_spam = false) as clean_count
#             FROM messages
#             WHERE created_at >= NOW() - INTERVAL '7 days'
#             GROUP BY DATE(created_at)
#             ORDER BY date
#         """)
#         trends = cursor.fetchall()
        
#         # Calculate detection accuracy by channel
#         cursor.execute("""
#             SELECT 
#                 type,
#                 AVG(CASE WHEN confidence >= 0.8 THEN 1 ELSE 0 END) * 100 as accuracy
#             FROM messages
#             GROUP BY type
#         """)
#         accuracy_by_channel = {row['type']: float(row['accuracy']) for row in cursor.fetchall()}
        
#         stats = {
#             'totalUsers': total_users,
#             'totalMessages': total_messages,
#             'spamCount': counts['spam_count'],
#             'hamCount': counts['ham_count'],
#             'messagesByType': {
#                 'email': messages_by_type.get('email', 0),
#                 'sms': messages_by_type.get('sms', 0),
#                 'social': messages_by_type.get('social', 0)
#             },
#             'recentActivity': [dict(activity) for activity in recent_activity],
#             'messagesByPeriod': [dict(trend) for trend in trends],
#             'accuracyByChannel': accuracy_by_channel,
#             'detectionRate': (counts['spam_count'] / total_messages * 100) if total_messages > 0 else 0
#         }
        
#         cursor.close()
#         conn.close()
        
#         return jsonify(stats), 200
        
#     except Exception as e:
#         print(f"Error fetching admin stats: {str(e)}")
#         return jsonify({
#             'error': 'Failed to fetch dashboard statistics',
#             'details': str(e)
#         }), 500

# @app.route('/api/admin/retrain', methods=['POST'])
# @token_required
# @admin_required
# def retrain_model(current_user):
#     """Endpoint to retrain the model"""
#     try:
#         global spam_model, text_preprocessor
        
#         print("Retraining model...")
#         spam_model, text_preprocessor = load_or_train_model()
        
#         return jsonify({
#             'message': 'Model retrained successfully',
#             'timestamp': datetime.utcnow().isoformat() + 'Z'
#         }), 200
        
#     except Exception as e:
#         return jsonify({'message': 'Retraining failed', 'error': str(e)}), 500

# @app.route('/api/admin/retrain-with-csv', methods=['POST'])
# @token_required
# @admin_required
# def retrain_with_csv(current_user):
#     try:
#         print("Starting CSV data retraining process...")
        
#         # Check if CSV files exist first
#         required_files = ['emails.csv', 'Bangla_Email_Dataset.csv', 'Dataset_5971.csv']
#         missing_files = [f for f in required_files if not os.path.exists(f)]
        
#         if missing_files:
#             raise Exception(f"Missing CSV files: {', '.join(missing_files)}")
        
#         # Force recreation of dataset
#         if os.path.exists('spam_dataset.csv'):
#             os.remove('spam_dataset.csv')
#             print("Removed existing dataset file")
        
#         # Create new dataset
#         df = create_enhanced_dataset()
#         if len(df) == 0:
#             raise Exception("Dataset is empty after loading CSV files")
            
#         print(f"Created new dataset with {len(df)} records")
        
#         # Train new model
#         global spam_model, text_preprocessor
#         spam_model, text_preprocessor = load_or_train_model()
        
#         return jsonify({
#             'message': 'Model retrained successfully',
#             'dataset_size': len(df),
#             'spam_count': int(sum(df['label'])),
#             'ham_count': int(len(df) - sum(df['label'])),
#             'timestamp': datetime.now(timezone.utc).isoformat()
#         }), 200
        
#     except Exception as e:
#         print(f"Retraining error: {str(e)}")
#         return jsonify({
#             'error': str(e)
#         }), 500

# @app.route('/api/admin/model-info', methods=['GET'])
# @token_required
# @admin_required
# def get_model_info(current_user):
#     """Get information about the current model"""
#     global spam_model, text_preprocessor
    
#     info = {
#         'model_loaded': spam_model is not None,
#         'preprocessor_loaded': text_preprocessor is not None,
#         'supported_languages': ['english', 'bangla', 'mixed'],
#         'supported_types': ['email', 'sms', 'social'],
#         'last_trained': datetime.utcnow().isoformat() + 'Z'
#     }
    
#     return jsonify(info), 200

# @app.route('/api/admin/check-csv-files', methods=['GET'])
# @token_required
# @admin_required
# def check_csv_files(current_user):
#     try:
#         required_files = ['emails.csv', 'Bangla_Email_Dataset.csv', 'Dataset_5971.csv']
#         missing_files = []
        
#         for file in required_files:
#             if not os.path.exists(file):
#                 missing_files.append(file)
        
#         return jsonify({
#             'exists': len(missing_files) == 0,
#             'missing_files': missing_files,
#             'working_directory': os.getcwd()
#         }), 200
        
#     except Exception as e:
#         return jsonify({
#             'error': str(e),
#             'message': 'Error checking CSV files'
#         }), 500

# @app.route('/api/admin/badge-counts', methods=['GET'])
# @token_required
# @admin_required
# def get_badge_counts(current_user):
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor(cursor_factory=RealDictCursor)
        
#         # Get new users in last 24 hours
#         cursor.execute("""
#             SELECT COUNT(*) as count 
#             FROM users 
#             WHERE created_at >= NOW() - INTERVAL '24 hours'
#         """)
#         new_users = cursor.fetchone()['count']
        
#         # Get new messages in last 24 hours
#         cursor.execute("""
#             SELECT COUNT(*) as count 
#             FROM messages 
#             WHERE created_at >= NOW() - INTERVAL '24 hours'
#         """)
#         new_messages = cursor.fetchone()['count']
        
#         cursor.close()
#         conn.close()
        
#         return jsonify({
#             'users': new_users,
#             'messages': new_messages
#         }), 200
        
#     except Exception as e:
#         print(f"Error fetching badge counts: {e}")
#         return jsonify({'users': 0, 'messages': 0}), 200
# # Add remaining admin routes (users, messages, etc.) - keeping them the same as in original
# @app.route('/api/admin/users', methods=['GET'])
# @token_required
# @admin_required
# def get_users(current_user):
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor(cursor_factory=RealDictCursor)
        
#         cursor.execute("SELECT id, name, email, role, created_at FROM users ORDER BY created_at DESC")
#         users = cursor.fetchall()
        
#         cursor.close()
#         conn.close()
        
#         return jsonify(users), 200
    
#     except Exception as e:
#         return jsonify({'message': 'Error fetching users', 'error': str(e)}), 500

# @app.route('/api/admin/messages', methods=['GET'])
# @token_required
# @admin_required
# def get_messages(current_user):
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor(cursor_factory=RealDictCursor)
        
#         query = """
#             SELECT m.id, m.content, m.type, m.language, m.is_spam, m.confidence, 
#                    m.created_at,
#                    json_build_object('id', u.id, 'name', u.name, 'email', u.email) as user
#             FROM messages m
#             JOIN users u ON m.user_id = u.id
#             ORDER BY m.created_at DESC
#             LIMIT 100
#         """
        
#         cursor.execute(query)
#         messages = cursor.fetchall()
        
#         # Convert decimal values to float for JSON serialization
#         for message in messages:
#             if 'confidence' in message and message['confidence'] is not None:
#                 message['confidence'] = float(message['confidence'])
        
#         cursor.close()
#         conn.close()
        
#         return jsonify(messages), 200
    
#     except Exception as e:
#         print(f"Error fetching messages: {e}")
#         return jsonify({
#             'error': 'Failed to fetch messages',
#             'details': str(e)
#         }), 500

# @app.route('/api/admin/users/<int:user_id>', methods=['DELETE'])
# @token_required
# @admin_required
# def delete_user(current_user, user_id):
#     # Prevent deleting your own account
#     if current_user['id'] == user_id:
#         return jsonify({'message': 'Cannot delete your own account'}), 400
    
#     try:
#         conn = get_db_connection()
#         cursor = conn.cursor(cursor_factory=RealDictCursor)
        
#         # Check if user exists and is not an admin
#         cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
#         user = cursor.fetchone()
        
#         if not user:
#             return jsonify({'message': 'User not found'}), 404
        
#         if user['role'] == 'admin':
#             return jsonify({'message': 'Cannot delete admin users'}), 403
        
#         # Delete user's messages first (due to foreign key constraint)
#         cursor.execute("DELETE FROM messages WHERE user_id = %s", (user_id,))
        
#         # Delete user
#         cursor.execute("DELETE FROM users WHERE id = %s", (user_id,))
        
#         conn.commit()
#         cursor.close()
#         conn.close()
        
#         return jsonify({'message': 'User deleted successfully'}), 200
    
#     except Exception as e:
#         return jsonify({'message': 'Error deleting user', 'error': str(e)}), 500

# @app.route('/api/admin/users', methods=['POST'])
# @token_required
# @admin_required
# def create_user():
#     try:
#         data = request.get_json()
#         name = data.get('name')
#         email = data.get('email')
#         password = data.get('password')
#         role = data.get('role', 'user')

#         if not all([name, email, password]):
#             return jsonify({'message': 'Missing required fields'}), 400

#         hashed_password = generate_password_hash(password)
        
#         conn = get_db_connection()
#         cursor = conn.cursor()
        
#         cursor.execute("""
#             INSERT INTO users (name, email, password, role)
#             VALUES (%s, %s, %s, %s)
#             RETURNING id, name, email, role, created_at
#         """, (name, email, hashed_password, role))
        
#         new_user = cursor.fetchone()
#         conn.commit()
#         cursor.close()
#         conn.close()

#         return jsonify({
#             'id': new_user[0],
#             'name': new_user[1],
#             'email': new_user[2],
#             'role': new_user[3],
#             'created_at': new_user[4]
#         }), 201
        
#     except Exception as e:
#         return jsonify({'message': str(e)}), 500

# # Health check endpoint
# @app.route('/api/health', methods=['GET'])
# def health_check():
#     global spam_model, text_preprocessor
    
#     return jsonify({
#         'status': 'healthy',
#         'model_loaded': spam_model is not None,
#         'preprocessor_loaded': text_preprocessor is not None,
#         'timestamp': datetime.utcnow().isoformat() + 'Z'
#     }), 200

# # Test prediction endpoint (no auth required for testing)
# @app.route('/api/test-predict', methods=['POST'])
# def test_predict():
#     """Test endpoint for prediction without authentication"""
#     data = request.get_json()
#     message = data.get('message', '')
#     message_type = data.get('type', 'email')
    
#     if not message:
#         return jsonify({'error': 'No message provided'}), 400
    
#     result = predict_message(message, message_type)
#     return jsonify(result), 200

# # Test cases
# test_messages = [
#     "‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶∞‡¶ø‡¶ö‡¶æ‡¶∞‡ßç‡¶ú ‡¶™‡ßá‡¶§‡ßá ‡¶ö‡¶æ‡¶®? ‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶® FREE ‡¶≤‡¶ø‡¶ñ‡ßá ‡ß©‡ß©‡ß©‡ß©‡ß© ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞‡ßá",
#     "‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø! ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶¨‡¶®‡ßç‡¶ß ‡¶π‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá‡•§ ‡¶è‡¶ñ‡¶®‡¶á ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®",
#     "‡¶Ü‡¶™‡¶®‡¶ø ‡ßß ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®! ‡¶è‡¶ñ‡¶®‡¶á ‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶® ‡ß¶‡ßß‡ß≠‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ",
#     "‡¶Ü‡¶ú‡¶ï‡ßá ‡¶∏‡¶®‡ßç‡¶ß‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶Ö‡¶´‡¶ø‡¶∏ ‡¶Æ‡¶ø‡¶ü‡¶ø‡¶Ç ‡¶Ü‡¶õ‡ßá",  # Normal message
# ]

# for msg in test_messages:
#     result = predict_message(msg)
#     print(f"\nMessage: {msg}")
#     print(f"Prediction: {'SPAM' if result['isSpam'] else 'HAM'}")
#     print(f"Confidence: {result['confidence']:.2%}")
#     print(f"Indicators: {result['indicators']}")

# # Initialize everything on startup
# def initialize_app():
#     """Initialize the application with database and model"""
#     global spam_model, text_preprocessor
    
#     print("üöÄ Initializing Enhanced Spam Detection App...")
    
#     try:
#         # Initialize database
#         print("üìä Initializing database...")
#         init_db()
#         print("‚úÖ Database initialized successfully")
        
#         # Load/train model
#         print("ü§ñ Loading/Training ML model...")
#         spam_model, text_preprocessor = load_or_train_model()
#         print("‚úÖ Model loaded successfully")
        
#         # Test the model
#         test_messages = [
#             ("Win free money now! Call 123-456-7890", "English spam"),
#             ("‡¶Ü‡¶™‡¶®‡¶ø ‡ßß ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®! ‡¶è‡¶ñ‡¶®‡¶á ‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®", "Bangla spam"), 
#             ("‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶∞‡¶ø‡¶ö‡¶æ‡¶∞‡ßç‡¶ú ‡¶™‡ßá‡¶§‡ßá ‡¶ö‡¶æ‡¶®? ‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶® FREE ‡¶≤‡¶ø‡¶ñ‡ßá ‡ß©‡ß©‡ß©‡ß©‡ß© ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞‡ßá", "Bangla spam"),
#             ("‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø! ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶¨‡¶®‡ßç‡¶ß ‡¶π‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá‡•§ ‡¶è‡¶ñ‡¶®‡¶á ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®", "Bangla spam"),
#             ("Hello, how are you doing today?", "English ham"),
#             ("‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ö‡¶´‡¶ø‡¶∏‡ßá ‡¶Ü‡¶∏‡¶¨‡ßá‡¶® ‡¶®‡¶æ ‡¶ï‡ßá‡¶®?", "Bangla ham")
#         ]
        
#         print("\nüß™ Testing model with sample messages:")
#         for msg, expected in test_messages:
#             result = predict_message(msg)
#             status = "‚úÖ" if (result['isSpam'] and "spam" in expected) or (not result['isSpam'] and "ham" in expected) else "‚ùå"
#             print(f"   {status} Message: '{msg}'")
#             print(f"       Expected: {expected}")
#             print(f"       Result: {'SPAM' if result['isSpam'] else 'HAM'} ({result['confidence']:.2%} confidence)")
#             print(f"       Language: {result['language']}")
        
#     except Exception as e:
#         print(f"‚ùå Error during initialization: {e}")
#         print("‚ö†Ô∏è  App may not function properly")

# # Application startup
# if __name__ == '__main__':
#     # Initialize the app
#     initialize_app()
    
#     # Run the Flask app
#     port = int(os.environ.get('PORT', 5000))
#     print(f"\nüåê Starting server on http://0.0.0.0:{port}")
#     print("Press Ctrl+C to stop the server")
    
#     app.run(host='0.0.0.0', port=port, debug=True)