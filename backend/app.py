import os
from datetime import datetime, timezone, timedelta
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.security import generate_password_hash, check_password_hash
import jwt.api_jwt as jwt
from functools import wraps
import psycopg2
from psycopg2.extras import RealDictCursor
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
import json
import re
import unicodedata
from collections import defaultdict

app = Flask(__name__)
# Configure CORS to allow requests from the frontend with all necessary headers
CORS(app, resources={
    r"/api/*": {
        "origins": ["http://localhost:5173", "http://localhost:3000"],
        "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
        "allow_headers": ["Content-Type", "Authorization"],
        "supports_credentials": True
    }
})

# Configuration
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev_key_please_change_in_production')
app.config['DATABASE_URL'] = os.environ.get(
    'DATABASE_URL',
    'postgresql://postgres:Mango%40292@localhost:5432/spam_detection'
)
app.config['JWT_EXPIRATION'] = int(os.environ.get('JWT_EXPIRATION', 86400))  # 24 hours

# Global model variables
spam_model = None
text_preprocessor = None
tfidf_vectorizer = None

# Database connection
def get_db_connection():
    conn = psycopg2.connect(app.config['DATABASE_URL'])
    conn.autocommit = True
    return conn

# Initialize database tables if they don't exist
def init_db():
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Create users table
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS users (
        id SERIAL PRIMARY KEY,
        name VARCHAR(100) NOT NULL,
        email VARCHAR(100) UNIQUE NOT NULL,
        password VARCHAR(255) NOT NULL,
        role VARCHAR(20) DEFAULT 'user',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    ''')
    
    # Create messages table with enhanced fields
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS messages (
        id SERIAL PRIMARY KEY,
        user_id INTEGER REFERENCES users(id),
        content TEXT NOT NULL,
        type VARCHAR(20) NOT NULL,
        language VARCHAR(10) DEFAULT 'unknown',
        is_spam BOOLEAN NOT NULL,
        confidence FLOAT NOT NULL,
        spam_indicators JSONB DEFAULT '{}',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    ''')
    
    # Check if admin exists, if not create one
    cursor.execute("SELECT * FROM users WHERE email = 'admin@example.com'")
    admin = cursor.fetchone()
    
    if not admin:
        admin_password = generate_password_hash('admin123')
        cursor.execute(
            "INSERT INTO users (name, email, password, role) VALUES (%s, %s, %s, %s)",
            ('Admin User', 'admin@example.com', admin_password, 'admin')
        )
    
    conn.commit()
    cursor.close()
    conn.close()

# Enhanced text preprocessing for multi-language support
class MultiLanguagePreprocessor:
    def __init__(self):
        # Common spam keywords in different languages
        self.spam_keywords = {
            'bangla': [
                # Existing ones
                '‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá', '‡¶´‡ßç‡¶∞‡¶ø', '‡¶ú‡¶ø‡¶§‡ßÅ‡¶®', '‡¶™‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡¶æ‡¶∞', '‡¶≤‡¶ü‡¶æ‡¶∞‡¶ø', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶Ö‡¶´‡¶æ‡¶∞',
                '‡¶õ‡¶æ‡¶°‡¶º', '‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶ï‡ßç‡¶≤‡¶ø‡¶ï', '‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø', '‡¶¨‡¶ø‡¶ú‡¶Ø‡¶º‡ßÄ',
                '‡¶≤‡¶æ‡¶≠‡¶ú‡¶®‡¶ï', '‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó', '‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§', '‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®', '‡¶™‡ßá‡¶§‡ßá',
                '‡¶∞‡¶ø‡¶ö‡¶æ‡¶∞‡ßç‡¶ú', '‡¶≤‡¶ï‡ßç‡¶∑', '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶ï‡ßã‡¶ü‡¶ø', '‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü', '‡¶¨‡¶®‡ßç‡¶ß', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ',
                '‡¶¨‡¶ø‡¶∂‡ßá‡¶∑', '‡¶°‡¶ø‡¶∏‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü', '‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤', '‡¶´‡ßã‡¶®', '‡¶®‡¶Æ‡ßç‡¶¨‡¶∞', '‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏',

                # New additions (more realistic spam triggers)
                '‡¶ï‡¶ø‡¶∏‡ßç‡¶§‡¶ø', '‡¶ã‡¶£', '‡¶á‡¶è‡¶Æ‡¶Ü‡¶á', '‡¶ß‡¶æ‡¶∞', '‡¶´‡¶æ‡¶®‡ßç‡¶°', '‡¶™‡ßç‡¶∞‡¶´‡¶ø‡¶ü',
                '‡¶°‡¶ø‡¶™‡ßã‡¶ú‡¶ø‡¶ü', '‡¶∏‡¶û‡ßç‡¶ö‡¶Ø‡¶º', '‡¶á‡¶®‡¶ï‡¶æ‡¶Æ', '‡¶Ü‡¶Ø‡¶º', '‡¶Ö‡¶∞‡ßç‡¶•',
                '‡¶™‡ßá‡¶Æ‡ßá‡¶®‡ßç‡¶ü', '‡¶≤‡ßá‡¶®‡¶¶‡ßá‡¶®', '‡¶ï‡ßç‡¶Ø‡¶æ‡¶∂', '‡¶°‡¶≤‡¶æ‡¶∞', '‡¶∞‡ßá‡¶Æ‡¶ø‡¶ü‡ßá‡¶®‡ßç‡¶∏',
                '‡¶ü‡¶æ‡¶ï‡¶æ ‡¶™‡¶æ‡¶†‡¶æ‡¶®', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞', '‡¶≠‡ßá‡¶∞‡¶ø‡¶´‡¶æ‡¶á', '‡¶™‡¶æ‡¶∏‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶°', '‡¶ì‡¶ü‡¶ø‡¶™‡¶ø',
                '‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§ ‡¶∏‡¶Æ‡¶Ø‡¶º', '‡¶è‡¶ï‡ßç‡¶∏‡¶ï‡ßç‡¶≤‡ßÅ‡¶∏‡¶ø‡¶≠', '‡¶¨‡ßã‡¶®‡¶æ‡¶∏', '‡¶™‡ßç‡¶∞‡¶æ‡¶á‡¶ú',
                '‡¶≤‡¶æ‡¶≠', '‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ', '‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø', '‡¶™‡¶æ‡¶∞‡ßç‡¶ü ‡¶ü‡¶æ‡¶á‡¶Æ', '‡¶Ü‡¶Ø‡¶º ‡¶ï‡¶∞‡ßÅ‡¶®',
                '‡¶∏‡ßã‡¶®‡¶æ', '‡¶π‡ßÄ‡¶∞‡¶æ‡¶∞', '‡¶™‡ßç‡¶∞‡¶™‡¶æ‡¶∞‡ßç‡¶ü‡¶ø', '‡¶™‡ßç‡¶≤‡¶ü', '‡¶´‡ßç‡¶≤‡ßç‡¶Ø‡¶æ‡¶ü',
                '‡¶∂‡ßá‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶∏‡ßç‡¶ü‡¶ï', '‡¶¨‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡ßã‡¶ó',
                '‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®', '‡¶≤‡¶æ‡¶á‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶∂‡ßá‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶∏‡¶æ‡¶¨‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶æ‡¶á‡¶¨',
                '‡¶≠‡¶ø‡¶∏‡¶æ', '‡¶Æ‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßç‡¶°', '‡¶ï‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ü ‡¶ï‡¶æ‡¶∞‡ßç‡¶°', '‡¶°‡ßá‡¶¨‡¶ø‡¶ü ‡¶ï‡¶æ‡¶∞‡ßç‡¶°',
                '‡¶≤‡¶ø‡¶Æ‡¶ø‡¶ü‡ßá‡¶° ‡¶Ö‡¶´‡¶æ‡¶∞', '‡¶´‡ßá‡¶ï', '‡¶Ö‡¶¨‡¶ø‡¶≤‡¶Æ‡ßç‡¶¨‡ßá', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶°',
                '‡¶Ö‡¶∞‡¶ø‡¶ú‡¶ø‡¶®‡¶æ‡¶≤ ‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü', '‡¶∏‡¶∏‡ßç‡¶§‡¶æ ‡¶¶‡¶æ‡¶Æ‡ßá', '‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡¶æ‡¶≤‡ßá‡¶ï‡¶∂‡¶®', '‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®',
                '‡ßß‡ß¶‡ß¶% ‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶Ü‡¶Ø‡¶º', '‡¶Ü‡¶ú‡¶ï‡ßá‡¶á', '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞',
                '‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡ßá ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø', '‡¶≠‡¶ø‡¶∏‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç', '‡¶´‡ßç‡¶∞‡¶ø ‡¶≠‡¶ø‡¶∏‡¶æ', '‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶ï ‡¶™‡¶æ‡¶∞‡¶Æ‡¶ø‡¶ü',
                '‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶´‡ßç‡¶∞‡¶ø', '‡¶∏‡¶æ‡¶∞‡ßç‡¶ü‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶ü', '‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø', '‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶Æ‡¶ø‡¶∂‡¶®', '‡¶´‡ßç‡¶∞‡¶ø ‡¶∞‡ßá‡¶ú‡¶ø‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡ßá‡¶∂‡¶®',
                '‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶®‡ßá‡¶ü ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï', '‡¶´‡ßç‡¶∞‡¶ø ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü', '‡¶´‡ßç‡¶∞‡¶ø ‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏', '‡¶´‡ßç‡¶∞‡¶ø ‡¶è‡¶Æ‡¶¨‡¶ø',
                '‡¶ú‡¶Ø‡¶º‡ßá‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶∏‡¶¶‡¶∏‡ßç‡¶Ø ‡¶π‡ßã‡¶®', '‡¶≠‡¶ø‡¶Ü‡¶á‡¶™‡¶ø ‡¶Ö‡¶´‡¶æ‡¶∞'
            ],
            'english': [
                'free', 'win', 'winner', 'cash', 'prize', 'urgent', 'offer',
                'limited', 'click', 'call now', 'guarantee', 'discount',
                'congratulation', 'lucky', 'selected', 'money', 'loan',
                'credit', 'debt', 'income', 'profit', 'bonus', 'reward',
                'exclusive', 'limited time', 'act now', 'hurry', 'today only',
                'investment', 'business opportunity', 'work from home', 'URGENT'
            ],
            'spanish': [
                'gratis', 'ganar', 'ganador', 'dinero', 'premio', 'urgente', 'oferta',
                'limitado', 'clic', 'llame ahora', 'garant√≠a', 'descuento',
                'felicitaciones', 'suerte', 'seleccionado', 'pr√©stamo', 'cr√©dito',
                'efectivo', 'bono', 'exclusivo', 'tiempo limitado', 'oportunidad',
                'trabaja desde casa', 'inversi√≥n', 'ganancias', 'promoci√≥n'
            ]
        }

        # Enhanced patterns for Bengali text
        self.patterns = {
            'phone': re.compile(r'(\+?88)?[-\s]?01[3-9]\d{8}|(\+?\d{1,3}[-.\s]?)?\(?\d{3,4}\)?[-.\s]?\d{3,4}[-.\s]?\d{3,6}'),
            'bangla_numbers': re.compile(r'[‡ß¶-‡ßØ]+'),
            'money_bangla': re.compile(r'‡ß≥\s*[\d‡ß¶-‡ßØ]+|[\d‡ß¶-‡ßØ]+\s*(‡¶ü‡¶æ‡¶ï‡¶æ|‡¶π‡¶æ‡¶ú‡¶æ‡¶∞|‡¶≤‡¶ï‡ßç‡¶∑|‡¶≤‡¶æ‡¶ñ|‡¶ï‡ßã‡¶ü‡¶ø)'),
            'money_english': re.compile(r'[$‚Ç¨¬£]\s*\d+|\d+\s*(?:dollars|euro|pound|USD|EUR|GBP)', re.IGNORECASE),
            'urgent_bangla': re.compile(r'(‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø|‡¶è‡¶ñ‡¶®‡¶á|‡¶§‡¶æ‡¶°‡¶º‡¶æ‡¶§‡¶æ‡¶°‡¶º‡¶ø|‡¶¶‡ßç‡¶∞‡ßÅ‡¶§|‡¶Ö‡¶¨‡¶ø‡¶≤‡¶Æ‡ßç‡¶¨‡ßá)', re.IGNORECASE),
            'urgent_english': re.compile(r'(urgent|now|hurry|limited time|act now|today only)', re.IGNORECASE),
            'urls': re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
        }

    def detect_language(self, text):
        """Enhanced language detection for Bengali text"""
        # Count Bengali characters
        bengali_chars = len(re.findall(r'[\u0980-\u09FF]', text))
        # Count Spanish characters
        spanish_chars = len(re.findall(r'[√°√©√≠√≥√∫√±¬ø¬°]', text.lower()))
        # Count English letters
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        # Count total meaningful characters
        total_chars = len(re.sub(r'[\s\d\W]', '', text))
        
        if total_chars == 0:
            return 'unknown'
        
        # Calculate ratios
        bengali_ratio = bengali_chars / max(total_chars, 1)
        spanish_ratio = spanish_chars / max(total_chars, 1)
        english_ratio = english_chars / max(total_chars, 1)
        
        # Determine language based on character ratios
        if bengali_ratio > 0.3:  # Lowered threshold for Bengali
            return 'bangla'
        elif spanish_ratio > 0.1:
            return 'spanish'
        elif english_ratio > 0.6:
            return 'english'
        elif bengali_ratio > 0.1 and english_ratio > 0.1:
            return 'mixed'
        else:
            return 'unknown'

    def extract_features(self, text):
        """Extract spam indicators from text"""
        features = defaultdict(int)
        text_lower = text.lower()
        
        # Language detection
        language = self.detect_language(text)
        features['language'] = language
        
        # Keyword matching
        for lang, keywords in self.spam_keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    features[f'{lang}_spam_keywords'] += 1
        
        # Pattern matching
        for pattern_name, pattern in self.patterns.items():
            matches = pattern.findall(text)
            features[f'{pattern_name}_count'] = len(matches)
        
        # Text statistics
        features['length'] = len(text)
        features['word_count'] = len(text.split())
        features['caps_ratio'] = len(re.findall(r'[A-Z]', text)) / max(len(text), 1)
        features['digit_ratio'] = len(re.findall(r'\d', text)) / max(len(text), 1)
        features['punctuation_ratio'] = len(re.findall(r'[!@#$%^&*(),.?":{}|<>]', text)) / max(len(text), 1)
        
        return dict(features)

    def preprocess_text(self, text):
        """Clean and normalize text"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Keep original text for multi-language TF-IDF (no Unicode normalization)
        return text

# Enhanced model training with multi-language support
def create_enhanced_dataset():
    """Create a comprehensive dataset by importing multiple CSV files"""
    print("Loading datasets from CSV files...")
    
    try:
        # Define required files and their column mappings
        datasets = [
            {
                'file': 'emails.csv',
                'text_col': 'Subject',
                'label_col': 'Label',
                'encoding': 'utf-8'
            },
            {
                'file': 'Bangla_Email_Dataset.csv',
                'text_col': 'Text',
                'label_col': 'Label',
                'encoding': 'utf-8'
            },
            {
                'file': 'Dataset_5971.csv',
                'text_col': 'Message',
                'label_col': 'Label',
                'encoding': 'utf-8'
            },
            {
                'file': 'spanish_spam.csv',  # Add your Spanish dataset
                'text_col': 'Mensaje',
                'label_col': 'Etiqueta',
                'encoding': 'utf-8'
            }
        ]
        
        combined_data = {
            'message': [],
            'label': [],
            'language': []
        }
        
        # Load each dataset
        for dataset in datasets:
            if os.path.exists(dataset['file']):
                try:
                    df = pd.read_csv(dataset['file'], encoding=dataset['encoding'])
                    print(f"‚úÖ Loaded {len(df)} records from {dataset['file']}")
                    
                    # Add messages and labels to combined data
                    for _, row in df.iterrows():
                        text = str(row[dataset['text_col']])
                        label = int(row[dataset['label_col']])
                        
                        # Detect language
                        preprocessor = MultiLanguagePreprocessor()
                        language = preprocessor.detect_language(text)
                        
                        combined_data['message'].append(text)
                        combined_data['label'].append(label)
                        combined_data['language'].append(language)
                        
                except Exception as e:
                    print(f"‚ùå Error loading {dataset['file']}: {e}")
            else:
                print(f"‚ö†Ô∏è File not found: {dataset['file']}")
        
        # Create DataFrame
        df = pd.DataFrame(combined_data)
        
        # Remove duplicates and null values
        df = df.drop_duplicates(subset=['message'])
        df = df.dropna()
        
        # Save combined dataset
        df.to_csv('spam_dataset.csv', index=False)
        
        print(f"\nüìä Dataset Statistics:")
        print(f"Total messages: {len(df)}")
        print(f"Spam messages: {sum(df['label'])}")
        print(f"Ham messages: {len(df) - sum(df['label'])}")
        print("\nLanguage distribution:")
        print(df['language'].value_counts())
        
        return df
        
    except Exception as e:
        print(f"‚ùå Error creating dataset: {e}")
        return pd.DataFrame()

def load_or_train_model():
    """Load or train model with multi-language support"""
    global tfidf_vectorizer
    print("Training enhanced multi-language spam detection model...")
    
    try:
        # Create or load dataset
        df = create_enhanced_dataset()
        
        if len(df) == 0:
            raise Exception("No data available for training")
        
        # Initialize preprocessor
        preprocessor = MultiLanguagePreprocessor()
        
        # Preprocess texts
        print("Preprocessing texts...")
        df['processed_text'] = df['message'].apply(preprocessor.preprocess_text)
        
        # Extract features
        print("Extracting features...")
        features_list = []
        for text in df['message']:
            features = preprocessor.extract_features(text)
            features_list.append(features)
        
        # Convert features to DataFrame
        features_df = pd.DataFrame(features_list).fillna(0)
        
        # Create TF-IDF vectorizer with multi-language support
        tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.9,
            analyzer='char_wb',  # Character n-grams for better multi-language support
            strip_accents=None,
            lowercase=True
        )
        
        # Create TF-IDF features
        tfidf_features = tfidf_vectorizer.fit_transform(df['processed_text'])
        
        # Combine features
        X = np.hstack([
            tfidf_features.toarray(),
            features_df.select_dtypes(include=[np.number]).values
        ])
        
        y = df['label']
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Train model with balanced class weights
        print("Training model...")
        model = RandomForestClassifier(
            n_estimators=200,  # Increased number of trees
            max_depth=20,
            min_samples_split=5,
            class_weight='balanced',
            random_state=42,
            n_jobs=-1
        )
        
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"\nüìà Model Performance:")
        print(f"Overall accuracy: {accuracy:.2%}")
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred))
        
        # Evaluate by language
        print("\nAccuracy by language:")
        for lang in df['language'].unique():
            mask = df.iloc[y_test.index]['language'] == lang
            if mask.sum() > 0:
                lang_acc = accuracy_score(y_test[mask], y_pred[mask])
                print(f"{lang}: {lang_acc:.2%}")
        
        return model, preprocessor
        
    except Exception as e:
        print(f"‚ùå Error during model training: {e}")
        return create_fallback_model()

def create_fallback_model():
    """Create a simple fallback model if main training fails"""
    global tfidf_vectorizer
    print("Creating fallback model...")
    
    # Simple model with basic data
    simple_data = {
        'message': [
            'win free money now', 'click here urgent', 'congratulations winner',
            'hello how are you', 'meeting at 3pm', 'thanks for help'
        ],
        'label': [1, 1, 1, 0, 0, 0]
    }
    
    df = pd.DataFrame(simple_data)
    
    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
    
    model = Pipeline([
        ('tfidf', tfidf_vectorizer),
        ('classifier', MultinomialNB())
    ])
    
    model.fit(df['message'], df['label'])
    preprocessor = MultiLanguagePreprocessor()
    
    return model, preprocessor

def calculate_spam_confidence(spam_score, indicators, is_spam):
    """Enhanced confidence calculation"""
    if is_spam:
        # For spam messages: confidence increases with more indicators
        base_confidence = 0.60
        
        # High-value indicators boost confidence more
        high_value_boost = 0
        if indicators.get('phone_numbers', 0) > 0:
            high_value_boost += 0.15
        if indicators.get('money_mentions', 0) > 0:
            high_value_boost += 0.12
        if indicators.get('urls', 0) > 0:
            high_value_boost += 0.10
        if indicators.get('urgent_words', 0) > 0:
            high_value_boost += 0.08
            
        # Keyword density matters
        keyword_boost = min(indicators.get('spam_keywords', 0) * 0.05, 0.15)
        
        confidence = base_confidence + high_value_boost + keyword_boost
        return min(confidence, 0.98)
    else:
        # For non-spam messages: higher confidence when fewer spam indicators
        if spam_score == 0:
            return 0.92  # Very confident for clean messages
        elif spam_score == 1:
            # Check what type of indicator it was
            if indicators.get('spam_keywords', 0) > 0 and indicators.get('phone_numbers', 0) == 0:
                return 0.75  # Keywords alone might be false positive
            else:
                return 0.65  # Other single indicators are more concerning
        elif spam_score == 2:
            return 0.58  # Close to spam threshold, low confidence
        else:
            return 0.52  # Very close to being classified as spam

# JWT Token Required Decorator
def token_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        token = None
        
        if 'Authorization' in request.headers:
            auth_header = request.headers['Authorization']
            if auth_header.startswith('Bearer '):
                token = auth_header[7:]
        
        if not token:
            return jsonify({'message': 'Token is missing!'}), 401
        
        try:
            data = jwt.decode(
                jwt=token,
                key=app.config['SECRET_KEY'],
                algorithms=["HS256"]
            )
            
            conn = get_db_connection()
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            cursor.execute("SELECT * FROM users WHERE id = %s", (data['user_id'],))
            current_user = cursor.fetchone()
            cursor.close()
            conn.close()
            
            if not current_user:
                return jsonify({'message': 'User no longer exists!'}), 401
            
        except Exception as e:
            return jsonify({'message': 'Token is invalid!', 'error': str(e)}), 401
        
        return f(current_user, *args, **kwargs)
    
    return decorated

# Admin Required Decorator
def admin_required(f):
    @wraps(f)
    def decorated(current_user, *args, **kwargs):
        if current_user['role'] != 'admin':
            return jsonify({'message': 'Admin privileges required!'}), 403
        return f(current_user, *args, **kwargs)
    
    return decorated

# Routes
@app.route('/api/auth/register', methods=['POST'])
def register():
    data = request.get_json()
    
    name = data.get('name')
    email = data.get('email')
    password = data.get('password')
    
    if not name or not email or not password:
        return jsonify({'message': 'Missing required fields'}), 400
    
    hashed_password = generate_password_hash(password)
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # Check if email already exists
        cursor.execute("SELECT * FROM users WHERE email = %s", (email,))
        if cursor.fetchone():
            return jsonify({'message': 'Email already registered'}), 409
        
        # Insert new user
        cursor.execute(
            "INSERT INTO users (name, email, password) VALUES (%s, %s, %s) RETURNING id",
            (name, email, hashed_password)
        )
        
        user_id = cursor.fetchone()[0]
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return jsonify({'message': 'User registered successfully', 'user_id': user_id}), 201
    
    except Exception as e:
        return jsonify({'message': 'Registration failed', 'error': str(e)}), 500

@app.route('/api/auth/login', methods=['POST'])
def login():
    data = request.get_json()
    
    email = data.get('email')
    password = data.get('password')
    
    if not email or not password:
        print(f"Login attempt failed: Missing email or password")
        return jsonify({'message': 'Missing email or password'}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        cursor.execute("SELECT * FROM users WHERE email = %s", (email,))
        user = cursor.fetchone()
        
        cursor.close()
        conn.close()
        
        if not user:
            print(f"Login attempt failed: No user found with email {email}")
            return jsonify({'message': 'Invalid credentials'}), 401
            
        if not check_password_hash(user['password'], password):
            print(f"Login attempt failed: Invalid password for user {email}")
            return jsonify({'message': 'Invalid credentials'}), 401
        
        # Generate JWT token with timezone-aware datetime
        token_payload = {
            'user_id': user['id'],
            'email': user['email'],
            'role': user['role'],
            'exp': datetime.now(timezone.utc) + timedelta(seconds=app.config['JWT_EXPIRATION'])
        }
        
        # Use jwt.encode with explicit algorithm
        token = jwt.encode(
            payload=token_payload,
            key=app.config['SECRET_KEY'],
            algorithm="HS256"
        )
        
        print(f"Successful login for user {email} with role {user['role']}")
        
        return jsonify({
            'token': token,
            'user': {
                'id': user['id'],
                'name': user['name'],
                'email': user['email'],
                'role': user['role']
            }
        }), 200
    
    except Exception as e:
        print(f"Login error: {str(e)}")
        return jsonify({'message': 'Login failed', 'error': str(e)}), 500

@app.route('/api/predict', methods=['POST'])
@token_required
def predict_spam(current_user):
    try:
        data = request.get_json()
        
        if not data or not data.get('message'):
            return jsonify({'error': 'No message provided'}), 400
            
        message = data.get('message')
        message_type = data.get('type', 'email')

        # Initialize preprocessor if needed
        global spam_model, text_preprocessor, tfidf_vectorizer
        
        # Check if ML model is available
        use_ml_model = spam_model is not None and text_preprocessor is not None and tfidf_vectorizer is not None
        
        if use_ml_model:
            try:
                # Use ML model for prediction
                processed_text = text_preprocessor.preprocess_text(message)
                features = text_preprocessor.extract_features(message)
                
                # Create TF-IDF features
                tfidf_features = tfidf_vectorizer.transform([processed_text])
                
                # Combine features (ensure consistent feature count)
                feature_values = []
                for key in sorted(features.keys()):
                    if isinstance(features[key], (int, float)):
                        feature_values.append(features[key])
                
                if hasattr(spam_model, 'predict_proba'):
                    # For scikit-learn models
                    X = np.hstack([tfidf_features.toarray(), np.array([feature_values])])
                    prediction = spam_model.predict(X)[0]
                    probabilities = spam_model.predict_proba(X)[0]
                    confidence = float(probabilities[1] if prediction else probabilities[0])
                else:
                    # For pipeline models
                    prediction = spam_model.predict([processed_text])[0]
                    probabilities = spam_model.predict_proba([processed_text])[0]
                    confidence = float(probabilities[1] if prediction else probabilities[0])
                
                is_spam = bool(prediction)
                language = text_preprocessor.detect_language(message)
                
                # Create indicators from features
                indicators = {
                    'spam_keywords': features.get('bangla_spam_keywords', 0) + features.get('english_spam_keywords', 0),
                    'phone_numbers': features.get('phone_count', 0),
                    'money_mentions': features.get('money_bangla_count', 0) + features.get('money_english_count', 0),
                    'urgent_words': features.get('urgent_bangla_count', 0) + features.get('urgent_english_count', 0),
                    'urls': features.get('urls_count', 0),
                    'text_length': features.get('length', len(message))
                }
                
                result = {
                    'isSpam': is_spam,
                    'confidence': confidence,
                    'message': 'Spam detected' if is_spam else 'Not spam',
                    'language': language,
                    'indicators': indicators
                }
                
            except Exception as ml_error:
                print(f"ML model error: {ml_error}, falling back to rule-based detection")
                use_ml_model = False
        
        if not use_ml_model:
            # Use rule-based detection as fallback
            print("Using rule-based detection")
            
            # Detect language and preprocess
            is_bangla = any('\u0980' <= c <= '\u09FF' for c in message)
            language = 'bangla' if is_bangla else 'english'
            
            # Spam detection logic
            spam_score = 0
            indicators = {
                'spam_keywords': 0,
                'phone_numbers': 0,
                'money_mentions': 0,
                'urgent_words': 0,
                'urls': 0,
                'text_length': len(message)
            }
            
            message_lower = message.lower()

            if is_bangla:
                # Bangla spam detection
                bangla_spam_keywords = [
                    '‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá', '‡¶´‡ßç‡¶∞‡¶ø', '‡¶ú‡¶ø‡¶§‡ßÅ‡¶®', '‡¶™‡ßÅ‡¶∞‡¶∏‡ßç‡¶ï‡¶æ‡¶∞', '‡¶≤‡¶ü‡¶æ‡¶∞‡¶ø', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶Ö‡¶´‡¶æ‡¶∞',
                    '‡¶õ‡¶æ‡¶°‡¶º', '‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶ï‡ßç‡¶≤‡¶ø‡¶ï', '‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®', '‡¶ó‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶®‡ßç‡¶ü‡¶ø', '‡¶¨‡¶ø‡¶ú‡¶Ø‡¶º‡ßÄ',
                    '‡¶≤‡¶æ‡¶≠‡¶ú‡¶®‡¶ï', '‡¶∏‡ßÅ‡¶Ø‡ßã‡¶ó', '‡¶∏‡ßÄ‡¶Æ‡¶ø‡¶§', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§', '‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§', '‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®', '‡¶™‡ßá‡¶§‡ßá',
                    '‡¶∞‡¶ø‡¶ö‡¶æ‡¶∞‡ßç‡¶ú', '‡¶≤‡¶ï‡ßç‡¶∑', '‡¶π‡¶æ‡¶ú‡¶æ‡¶∞', '‡¶ï‡ßã‡¶ü‡¶ø', '‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü', '‡¶¨‡¶®‡ßç‡¶ß', '‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ'
                ]
                
                # Check for Bangla spam keywords
                for keyword in bangla_spam_keywords:
                    if keyword.lower() in message_lower:
                        spam_score += 1
                        indicators['spam_keywords'] += 1

                # Check for phone numbers
                phone_patterns = [
                    r'(\+?88)?[-\s]?01[3-9]\d{8}',  # Bangla mobile numbers
                    r'[‡ß¶-‡ßØ]{11}'  # Bangla numeric mobile numbers
                ]
                for pattern in phone_patterns:
                    if re.search(pattern, message):
                        spam_score += 2  # Phone numbers are strong indicators
                        indicators['phone_numbers'] += 1

                # Check for money mentions
                money_patterns = [
                    r'‡ß≥\s*[\d‡ß¶-‡ßØ]+',
                    r'[\d‡ß¶-‡ßØ]+\s*(‡¶ü‡¶æ‡¶ï‡¶æ|‡¶π‡¶æ‡¶ú‡¶æ‡¶∞|‡¶≤‡¶ï‡ßç‡¶∑|‡¶≤‡¶æ‡¶ñ|‡¶ï‡ßã‡¶ü‡¶ø)'
                ]
                for pattern in money_patterns:
                    if re.search(pattern, message):
                        spam_score += 2  # Money mentions are strong indicators
                        indicators['money_mentions'] += 1

                # Check for urgent words
                urgent_words = ['‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶§‡¶æ‡¶°‡¶º‡¶æ‡¶§‡¶æ‡¶°‡¶º‡¶ø', '‡¶¶‡ßç‡¶∞‡ßÅ‡¶§', '‡¶Ö‡¶¨‡¶ø‡¶≤‡¶Æ‡ßç‡¶¨‡ßá']
                for word in urgent_words:
                    if word.lower() in message_lower:
                        spam_score += 1
                        indicators['urgent_words'] += 1

            else:
                # English spam detection
                english_spam_keywords = [
                    'free', 'win', 'winner', 'cash', 'prize', 'urgent', 'offer',
                    'limited', 'click', 'call now', 'guarantee', 'discount',
                    'congratulation', 'lucky', 'selected', 'money', 'loan',
                    'credit', 'debt', 'bonus', 'reward', 'exclusive'
                ]
                
                # Check for English spam keywords
                for keyword in english_spam_keywords:
                    if keyword.lower() in message_lower:
                        spam_score += 1
                        indicators['spam_keywords'] += 1

                # Check for phone numbers
                if re.search(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', message):
                    spam_score += 2  # Phone numbers are strong indicators
                    indicators['phone_numbers'] += 1

                # Check for money mentions
                if re.search(r'[$‚Ç¨¬£]\s*\d+|\d+\s*(?:dollars|euro|pound|USD|EUR|GBP)', message_lower):
                    spam_score += 2  # Money mentions are strong indicators
                    indicators['money_mentions'] += 1

                # Check for urgent words
                urgent_words = ['urgent', 'now', 'hurry', 'limited time', 'act now', 'today only']
                for word in urgent_words:
                    if word.lower() in message_lower:
                        spam_score += 1
                        indicators['urgent_words'] += 1

            # Check for URLs in both languages
            if re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', message):
                spam_score += 2  # URLs are strong indicators
                indicators['urls'] += 1

            # Enhanced spam determination logic
            if spam_score >= 4:
                is_spam = True
            elif spam_score >= 3:
                # Check if high-value indicators are present
                high_value_indicators = indicators['phone_numbers'] + indicators['money_mentions'] + indicators['urls']
                is_spam = high_value_indicators >= 1
            elif spam_score >= 2:
                # Need at least one high-value indicator
                high_value_indicators = indicators['phone_numbers'] + indicators['money_mentions'] + indicators['urls']
                is_spam = high_value_indicators >= 1 and indicators['spam_keywords'] >= 1
            else:
                is_spam = False

            # Calculate confidence using the enhanced function
            confidence = calculate_spam_confidence(spam_score, indicators, is_spam)

            result = {
                'isSpam': is_spam,
                'confidence': confidence,
                'message': 'Spam detected' if is_spam else 'Not spam',
                'language': language,
                'indicators': indicators
            }

        # Save to database
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute(
                """INSERT INTO messages 
                   (user_id, content, type, language, is_spam, confidence, spam_indicators) 
                   VALUES (%s, %s, %s, %s, %s, %s, %s)""",
                (current_user['id'], message, message_type, result['language'],
                 result['isSpam'], result['confidence'], json.dumps(result['indicators']))
            )
            conn.commit()
            cursor.close()
            conn.close()
        except Exception as db_error:
            print(f"Database error: {db_error}")

        return jsonify(result), 200

    except Exception as e:
        print(f"Prediction error: {str(e)}")
        return jsonify({
            'error': 'Error detecting spam',
            'details': str(e)
        }), 500

# Admin routes
@app.route('/api/admin/stats', methods=['GET'])
@token_required
@admin_required
def admin_stats(current_user):
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # Get total users
        cursor.execute("SELECT COUNT(*) as total FROM users")
        total_users = cursor.fetchone()['total']
        
        # Get total messages
        cursor.execute("SELECT COUNT(*) as total FROM messages")
        total_messages = cursor.fetchone()['total']
        
        # Get spam and ham counts
        cursor.execute("""
            SELECT 
                COUNT(*) FILTER (WHERE is_spam = true) as spam_count,
                COUNT(*) FILTER (WHERE is_spam = false) as ham_count
            FROM messages
        """)
        counts = cursor.fetchone()
        
        # Get messages by type
        cursor.execute("""
            SELECT type, COUNT(*) as count 
            FROM messages 
            GROUP BY type
        """)
        messages_by_type = {row['type']: row['count'] for row in cursor.fetchall()}
        
        # Get recent activity
        cursor.execute("""
            SELECT 
                m.id,
                m.content,
                m.type,
                m.is_spam,
                m.confidence,
                m.created_at,
                json_build_object(
                    'id', u.id,
                    'name', u.name,
                    'email', u.email
                ) as user
            FROM messages m
            JOIN users u ON m.user_id = u.id
            ORDER BY m.created_at DESC
            LIMIT 10
        """)
        recent_activity = cursor.fetchall()
        
        # Get message trends (last 7 days)
        cursor.execute("""
            SELECT 
                DATE(created_at) as date,
                COUNT(*) FILTER (WHERE is_spam = true) as spam_count,
                COUNT(*) FILTER (WHERE is_spam = false) as clean_count
            FROM messages
            WHERE created_at >= NOW() - INTERVAL '7 days'
            GROUP BY DATE(created_at)
            ORDER BY date
        """)
        trends = cursor.fetchall()
        
        # Calculate detection accuracy by channel
        cursor.execute("""
            SELECT 
                type,
                AVG(CASE WHEN confidence >= 0.8 THEN 1 ELSE 0 END) * 100 as accuracy
            FROM messages
            GROUP BY type
        """)
        accuracy_by_channel = {row['type']: float(row['accuracy']) for row in cursor.fetchall()}
        
        stats = {
            'totalUsers': total_users,
            'totalMessages': total_messages,
            'spamCount': counts['spam_count'],
            'hamCount': counts['ham_count'],
            'messagesByType': {
                'email': messages_by_type.get('email', 0),
                'sms': messages_by_type.get('sms', 0),
                'social': messages_by_type.get('social', 0)
            },
            'recentActivity': [dict(activity) for activity in recent_activity],
            'messagesByPeriod': [dict(trend) for trend in trends],
            'accuracyByChannel': accuracy_by_channel,
            'detectionRate': (counts['spam_count'] / total_messages * 100) if total_messages > 0 else 0
        }
        
        cursor.close()
        conn.close()
        
        return jsonify(stats), 200
        
    except Exception as e:
        print(f"Error fetching admin stats: {str(e)}")
        return jsonify({
            'error': 'Failed to fetch dashboard statistics',
            'details': str(e)
        }), 500

@app.route('/api/admin/retrain', methods=['POST'])
@token_required
@admin_required
def retrain_model(current_user):
    """Endpoint to retrain the model"""
    try:
        global spam_model, text_preprocessor, tfidf_vectorizer
        
        print("Retraining model...")
        spam_model, text_preprocessor = load_or_train_model()
        
        return jsonify({
            'message': 'Model retrained successfully',
            'timestamp': datetime.now(timezone.utc).isoformat()
        }), 200
        
    except Exception as e:
        return jsonify({'message': 'Retraining failed', 'error': str(e)}), 500

@app.route('/api/admin/retrain-with-csv', methods=['POST'])
@token_required
@admin_required
def retrain_with_csv(current_user):
    try:
        print("Starting CSV data retraining process...")
        
        # Check if CSV files exist first
        required_files = ['emails.csv', 'Bangla_Email_Dataset.csv', 'Dataset_5971.csv', 'spam_dataset.csv']
        missing_files = [f for f in required_files if not os.path.exists(f)]
        
        if missing_files:
            raise Exception(f"Missing CSV files: {', '.join(missing_files)}")
        
        # Force recreation of dataset
        if os.path.exists('spam_dataset.csv'):
            os.remove('spam_dataset.csv')
            print("Removed existing dataset file")
        
        # Create new dataset
        df = create_enhanced_dataset()
        if len(df) == 0:
            raise Exception("Dataset is empty after loading CSV files")
            
        print(f"Created new dataset with {len(df)} records")
        
        # Train new model
        global spam_model, text_preprocessor, tfidf_vectorizer
        spam_model, text_preprocessor = load_or_train_model()
        
        return jsonify({
            'message': 'Model retrained successfully with CSV data',
            'dataset_size': len(df),
            'spam_count': int(sum(df['label'])),
            'ham_count': int(len(df) - sum(df['label'])),
            'timestamp': datetime.now(timezone.utc).isoformat()
        }), 200
        
    except Exception as e:
        print(f"Retraining error: {str(e)}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/api/admin/model-info', methods=['GET'])
@token_required
@admin_required
def get_model_info(current_user):
    """Get information about the current model"""
    global spam_model, text_preprocessor, tfidf_vectorizer
    
    info = {
        'model_loaded': spam_model is not None,
        'preprocessor_loaded': text_preprocessor is not None,
        'tfidf_loaded': tfidf_vectorizer is not None,
        'supported_languages': ['english', 'bangla', 'mixed'],
        'supported_types': ['email', 'sms', 'social'],
        'last_trained': datetime.now(timezone.utc).isoformat()
    }
    
    return jsonify(info), 200

@app.route('/api/admin/check-csv-files', methods=['GET'])
@token_required
@admin_required
def check_csv_files(current_user):
    try:
        required_files = ['emails.csv', 'Bangla_Email_Dataset.csv', 'Dataset_5971.csv']
        missing_files = []
        existing_files = []
        
        for file in required_files:
            if not os.path.exists(file):
                missing_files.append(file)
            else:
                existing_files.append(file)
        
        return jsonify({
            'all_files_exist': len(missing_files) == 0,
            'existing_files': existing_files,
            'missing_files': missing_files,
            'working_directory': os.getcwd()
        }), 200
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'message': 'Error checking CSV files'
        }), 500

@app.route('/api/admin/badge-counts', methods=['GET'])
@token_required
@admin_required
def get_badge_counts(current_user):
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # Get new users in last 24 hours
        cursor.execute("""
            SELECT COUNT(*) as count 
            FROM users 
            WHERE created_at >= NOW() - INTERVAL '24 hours'
        """)
        new_users = cursor.fetchone()['count']
        
        # Get new messages in last 24 hours
        cursor.execute("""
            SELECT COUNT(*) as count 
            FROM messages 
            WHERE created_at >= NOW() - INTERVAL '24 hours'
        """)
        new_messages = cursor.fetchone()['count']
        
        cursor.close()
        conn.close()
        
        return jsonify({
            'users': new_users,
            'messages': new_messages
        }), 200
        
    except Exception as e:
        print(f"Error fetching badge counts: {e}")
        return jsonify({'users': 0, 'messages': 0}), 200

@app.route('/api/admin/users', methods=['GET'])
@token_required
@admin_required
def get_users(current_user):
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        cursor.execute("SELECT id, name, email, role, created_at FROM users ORDER BY created_at DESC")
        users = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        return jsonify(users), 200
    
    except Exception as e:
        return jsonify({'message': 'Error fetching users', 'error': str(e)}), 500

@app.route('/api/admin/messages', methods=['GET'])
@token_required
@admin_required
def get_messages(current_user):
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        query = """
            SELECT m.id, m.content, m.type, m.language, m.is_spam, m.confidence, 
                   m.created_at,
                   json_build_object('id', u.id, 'name', u.name, 'email', u.email) as user
            FROM messages m
            JOIN users u ON m.user_id = u.id
            ORDER BY m.created_at DESC
            LIMIT 100
        """
        
        cursor.execute(query)
        messages = cursor.fetchall()
        
        # Convert decimal values to float for JSON serialization
        for message in messages:
            if 'confidence' in message and message['confidence'] is not None:
                message['confidence'] = float(message['confidence'])
        
        cursor.close()
        conn.close()
        
        return jsonify(messages), 200
    
    except Exception as e:
        print(f"Error fetching messages: {e}")
        return jsonify({
            'error': 'Failed to fetch messages',
            'details': str(e)
        }), 500

@app.route('/api/admin/users/<int:user_id>', methods=['DELETE'])
@token_required
@admin_required
def delete_user(current_user, user_id):
    # Prevent deleting your own account
    if current_user['id'] == user_id:
        return jsonify({'message': 'Cannot delete your own account'}), 400
    
    try:
        conn = get_db_connection()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # Check if user exists and is not an admin
        cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
        user = cursor.fetchone()
        
        if not user:
            return jsonify({'message': 'User not found'}), 404
        
        if user['role'] == 'admin':
            return jsonify({'message': 'Cannot delete admin users'}), 403
        
        # Delete user's messages first (due to foreign key constraint)
        cursor.execute("DELETE FROM messages WHERE user_id = %s", (user_id,))
        
        # Delete user
        cursor.execute("DELETE FROM users WHERE id = %s", (user_id,))
        
        conn.commit()
        cursor.close()
        conn.close()
        
        return jsonify({'message': 'User deleted successfully'}), 200
    
    except Exception as e:
        return jsonify({'message': 'Error deleting user', 'error': str(e)}), 500

@app.route('/api/admin/users', methods=['POST'])
@token_required
@admin_required
def create_user(current_user):
    try:
        data = request.get_json()
        name = data.get('name')
        email = data.get('email')
        password = data.get('password')
        role = data.get('role', 'user')

        if not all([name, email, password]):
            return jsonify({'message': 'Missing required fields'}), 400

        hashed_password = generate_password_hash(password)
        
        conn = get_db_connection()
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO users (name, email, password, role)
            VALUES (%s, %s, %s, %s)
            RETURNING id, name, email, role, created_at
        """, (name, email, hashed_password, role))
        
        new_user = cursor.fetchone()
        conn.commit()
        cursor.close()
        conn.close()

        return jsonify({
            'id': new_user[0],
            'name': new_user[1],
            'email': new_user[2],
            'role': new_user[3],
            'created_at': new_user[4]
        }), 201
        
    except Exception as e:
        return jsonify({'message': str(e)}), 500

# Health check endpoint
@app.route('/api/health', methods=['GET'])
def health_check():
    global spam_model, text_preprocessor, tfidf_vectorizer
    
    return jsonify({
        'status': 'healthy',
        'model_loaded': spam_model is not None,
        'preprocessor_loaded': text_preprocessor is not None,
        'tfidf_loaded': tfidf_vectorizer is not None,
        'timestamp': datetime.now(timezone.utc).isoformat()
    }), 200

# Test prediction endpoint (no auth required for testing)
@app.route('/api/test-predict', methods=['POST'])
def test_predict():
    """Test endpoint for prediction without authentication"""
    data = request.get_json()
    message = data.get('message', '')
    message_type = data.get('type', 'email')
    
    if not message:
        return jsonify({'error': 'No message provided'}), 400
    
    # Mock current_user for testing
    mock_user = {'id': 1}
    
    try:
        # Call the same prediction logic as the main endpoint
        return predict_spam(mock_user)
    except Exception as e:
        return jsonify({
            'error': 'Test prediction failed',
            'details': str(e)
        }), 500

# Initialize everything on startup
def initialize_app():
    """Initialize the application with database and model"""
    global spam_model, text_preprocessor, tfidf_vectorizer
    
    print("üöÄ Initializing Enhanced Spam Detection App...")
    
    try:
        # Initialize database
        print("üìä Initializing database...")
        init_db()
        print("‚úÖ Database initialized successfully")
        
        # Load/train model
        print("ü§ñ Loading/Training ML model...")
        spam_model, text_preprocessor = load_or_train_model()
        print("‚úÖ Model loaded successfully")
        
        # Test the model
        test_messages = [
            ("Win free money now! Call 123-456-7890", "English spam"),
            ("‡¶Ü‡¶™‡¶®‡¶ø ‡ßß ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶ú‡¶ø‡¶§‡ßá‡¶õ‡ßá‡¶®! ‡¶è‡¶ñ‡¶®‡¶á ‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®", "Bangla spam"), 
            ("‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶∞‡¶ø‡¶ö‡¶æ‡¶∞‡ßç‡¶ú ‡¶™‡ßá‡¶§‡ßá ‡¶ö‡¶æ‡¶®? ‡¶è‡¶∏‡¶è‡¶Æ‡¶è‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶® FREE ‡¶≤‡¶ø‡¶ñ‡ßá ‡ß©‡ß©‡ß©‡ß©‡ß© ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞‡ßá", "Bangla spam"),
            ("‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø! ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶¨‡¶®‡ßç‡¶ß ‡¶π‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá‡•§ ‡¶è‡¶ñ‡¶®‡¶á ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®", "Bangla spam"),
            ("Hello, how are you doing today?", "English ham"),
            ("‡¶Ü‡¶ú‡¶ï‡ßá ‡¶Ö‡¶´‡¶ø‡¶∏‡ßá ‡¶Ü‡¶∏‡¶¨‡ßá‡¶® ‡¶®‡¶æ ‡¶ï‡ßá‡¶®?", "Bangla ham"),
            ("Meeting at 3 PM tomorrow", "English ham"),
            ("‡¶ï‡¶æ‡¶≤ ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶Ü‡¶õ‡ßá", "Bangla ham")
        ]
        
        print("\nüß™ Testing model with sample messages:")
        for msg, expected in test_messages:
            # Use rule-based prediction for testing
            is_bangla = any('\u0980' <= c <= '\u09FF' for c in msg)
            spam_score = 0
            
            if is_bangla:
                bangla_keywords = ['‡¶¨‡¶ø‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡ßç‡¶Ø‡ßá', '‡¶´‡ßç‡¶∞‡¶ø', '‡¶ú‡¶ø‡¶§‡ßÅ‡¶®', '‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø', '‡¶è‡¶ñ‡¶®‡¶á', '‡¶ü‡¶æ‡¶ï‡¶æ', '‡¶ï‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®']
                for kw in bangla_keywords:
                    if kw in msg:
                        spam_score += 1
            else:
                english_keywords = ['free', 'win', 'money', 'call', 'urgent', 'now']
                for kw in english_keywords:
                    if kw.lower() in msg.lower():
                        spam_score += 1
                        
            # Check for phone numbers
            if re.search(r'(\d{3}[-.]?\d{3}[-.]?\d{4}|01[3-9]\d{8})', msg):
                spam_score += 2
                
            is_spam = spam_score >= 2
            confidence = calculate_spam_confidence(spam_score, {'spam_keywords': spam_score}, is_spam)
            
            status = "‚úÖ" if (is_spam and "spam" in expected) or (not is_spam and "ham" in expected) else "‚ùå"
            print(f"   {status} Message: '{msg[:50]}{'...' if len(msg) > 50 else ''}'")
            print(f"       Expected: {expected}")
            print(f"       Result: {'SPAM' if is_spam else 'HAM'} ({confidence:.1%} confidence)")
            print(f"       Score: {spam_score}")
        
    except Exception as e:
        print(f"‚ùå Error during initialization: {e}")
        print("‚ö†Ô∏è  App may not function properly, but rule-based detection will work")

# Application startup
if __name__ == '__main__':
    # Initialize the app
    initialize_app()
    
    # Run the Flask app
    port = int(os.environ.get('PORT', 5000))
    print(f"\nüåê Starting server on http://0.0.0.0:{port}")
    print("üìù Admin credentials: admin@example.com / admin123")
    print("Press Ctrl+C to stop the server")
    
    app.run(host='0.0.0.0', port=port, debug=True)
